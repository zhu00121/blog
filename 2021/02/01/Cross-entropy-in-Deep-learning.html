<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Cross Entropy In Deep Learning | Yi’ 1st blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Cross Entropy In Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Why CE?" />
<meta property="og:description" content="Why CE?" />
<link rel="canonical" href="https://zhu00121.github.io/blog/2021/02/01/Cross-entropy-in-Deep-learning.html" />
<meta property="og:url" content="https://zhu00121.github.io/blog/2021/02/01/Cross-entropy-in-Deep-learning.html" />
<meta property="og:site_name" content="Yi’ 1st blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://zhu00121.github.io/blog/2021/02/01/Cross-entropy-in-Deep-learning.html","@type":"BlogPosting","headline":"Cross Entropy In Deep Learning","dateModified":"2021-02-01T00:00:00-06:00","datePublished":"2021-02-01T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhu00121.github.io/blog/2021/02/01/Cross-entropy-in-Deep-learning.html"},"description":"Why CE?","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zhu00121.github.io/blog/feed.xml" title="Yi' 1st blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Yi&#39; 1st blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Cross Entropy In Deep Learning</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="why-ce"><strong>Why CE?</strong></h3>

<p>I remember when I first saw the phrase ‘cross-entropy (or CE for short)’, I had no idea what it meant. But it seemed to exist in almost every ML/DL paper, where people use it as the loss function of their model. After reading some articles explaining this loss function, I still found it not straightforward enough (considering I’m not a Math/CS student), so I decided to write something about it and try to explain it from a more intuitive perspective.</p>

<h3 id="what-is-ce"><strong>What is CE?</strong></h3>

<p>To make it simple, it’s just one of the many loss functions (but the most popular one), with which we use to update the parameters of our model.</p>

<p>Notice that loss is different from error rate. Error rate directly answers the question “How many mistakes did the model make (in percentage)?”. While loss function could be considered as something to approximate error rate, but is used to update parameters (machine uses this loss function to ‘learn’ by itself).</p>

<h3 id="where-does-ce-fit-in-the-dl-pipeline"><strong>Where does CE fit in the DL pipeline?</strong></h3>

<p>Before I show an example of calculating CE, I would like to show you where this CE sits in our model and what it does in general.</p>

<p><img src="assets/img/2021-02-01-Cross-entropy-in-Deep-learning/media/image1.jpeg" alt="CE" /></p>

<p>The CE function sits right after Softmax function (which is used to calculate the probabilities of this sample for being each of the classes in a classification problem). It has two inputs: outputs from Softmax function and the one-hot encodings of the targets.</p>

<h3 id="details-of-ce">Details of CE?</h3>

<p>As mentioned above, CE is probably the most popular loss function that is used in a classification problem.Thus, looking into the details of CE would help us understand a lot of models proposed by other people and make modifications if needed.</p>

<p><strong>Here goes an example.</strong></p>

<p>Suppose we need to classify an image into one of the three classes (e.g. cat/dog/human). After we feed this image into our inner layers of the model and the Softmax function, it gives us an output [0.8, 0.1, 0.1], where the three numbers respectively represent the probability of this image being a cat/dog/human. Now we know that the actual label (I.e. target) is [1,0,0] (in a 1-hot-encoded way), the question now would be “How do we figure out a way to quantify the difference between predictions and targets?”</p>

<p><img src="assets/img/2021-02-01-Cross-entropy-in-Deep-learning/media/image2.jpeg" alt="CE2" /></p>

<p>A simple way to do is calculate the dot product of these two vectors. The idea of multiplying these two vectors would give us a results that represents how similar they are.</p>

<p>Since: 0.8*1=1, 0.1*0=0, 0.1*0=0, the dot product is now [1,0,0]. So we have a way to measure their similarity, but how do we measure the difference? To use Cross-entropy!</p>

<p><strong>Here is how we calculate CE:</strong></p>

<p><img src="assets/img/2021-02-01-Cross-entropy-in-Deep-learning/media/image3.jpeg" alt="CE3" /></p>

<p>Pi is the output of the Softmax function and Ti is the target. What CE does is that it multiplies the log of each number in the prediction with target and add them all up. Since the log usually makes it negative, we have a ‘-’ at front to make the number positive.</p>

<p>Additionally, we could take the average of this CE(P,T) so that even when we have a large number of data points (I.e. when i is large), we still end up having a small number of cross-entropy.</p>

  </div><a class="u-url" href="/blog/2021/02/01/Cross-entropy-in-Deep-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This blog records my personal experience in ML/DL</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
